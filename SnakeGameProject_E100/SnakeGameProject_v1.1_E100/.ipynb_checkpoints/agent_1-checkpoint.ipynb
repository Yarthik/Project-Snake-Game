{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "###agent_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broad-spoke",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from snake_env.ipynb\n",
      "importing Jupyter notebook from plot_script.ipynb\n",
      "final state before dying: [[1 1 0 0 0 0 1 0 0 1 0 0]]\n",
      "episode: 1/50, score: -75\n",
      "final state before dying: [[0 1 0 0 0 0 0 1 0 0 0 1]]\n",
      "episode: 2/50, score: -105\n",
      "final state before dying: [[0 1 1 0 1 0 0 0 1 0 0 0]]\n",
      "episode: 3/50, score: -94\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 0 0 1 0]]\n",
      "episode: 4/50, score: -109\n",
      "final state before dying: [[0 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 5/50, score: -99\n",
      "final state before dying: [[0 0 1 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 6/50, score: -98\n",
      "final state before dying: [[0 0 1 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 7/50, score: -80\n",
      "final state before dying: [[0 1 0 0 0 0 1 0 0 0 1 0]]\n",
      "episode: 8/50, score: -98\n",
      "final state before dying: [[0 1 0 0 0 0 1 0 0 0 0 1]]\n",
      "episode: 9/50, score: -91\n",
      "final state before dying: [[0 0 1 1 0 0 0 1 0 0 1 0]]\n",
      "episode: 10/50, score: 270\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 1 0 0 0]]\n",
      "episode: 11/50, score: 340\n",
      "final state before dying: [[1 1 0 0 1 0 0 0 0 1 0 0]]\n",
      "episode: 12/50, score: 452\n",
      "final state before dying: [[0 1 0 0 0 1 1 0 0 1 0 0]]\n",
      "episode: 13/50, score: 419\n",
      "final state before dying: [[0 0 0 1 0 0 0 1 0 0 1 0]]\n",
      "episode: 14/50, score: 457\n",
      "final state before dying: [[1 0 0 0 1 0 0 0 1 0 0 0]]\n",
      "episode: 15/50, score: 505\n",
      "final state before dying: [[0 0 1 1 1 0 0 0 1 0 0 0]]\n",
      "episode: 16/50, score: 378\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-81d7260f1a77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m#     env = Snake(env_info=env_info)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0msum_of_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_of_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-81d7260f1a77>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[1;34m(episode, env)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;31m# print(action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Jupyter_Workspace\\SnakeGameProject100Episodes\\SnakeGameProject_v1.1\\snake_env.ipynb\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n",
      "\u001b[1;32mD:\\Jupyter_Workspace\\SnakeGameProject100Episodes\\SnakeGameProject_v1.1\\snake_env.ipynb\u001b[0m in \u001b[0;36mrun_game\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;32mD:\\Jupyter_Workspace\\SnakeGameProject100Episodes\\SnakeGameProject_v1.1\\snake_env.ipynb\u001b[0m in \u001b[0;36mmove_snake\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\TensorFlow\\lib\\turtle.py\u001b[0m in \u001b[0;36msetx\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1807\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;36m10.00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m240.00\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m         \"\"\"\n\u001b[1;32m-> 1809\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_goto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVec2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_position\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msety\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\TensorFlow\\lib\\turtle.py\u001b[0m in \u001b[0;36m_goto\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m   3157\u001b[0m                       (self.currentLineItem,\n\u001b[0;32m   3158\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentLine\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3159\u001b[1;33m                       \u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pointlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentLineItem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3160\u001b[0m                       self.items[:])\n\u001b[0;32m   3161\u001b[0m                       )\n",
      "\u001b[1;32mD:\\Anaconda3\\TensorFlow\\lib\\turtle.py\u001b[0m in \u001b[0;36m_pointlist\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    754\u001b[0m         (9.9999999999999982, 0.0)]\n\u001b[0;32m    755\u001b[0m         >>> \"\"\"\n\u001b[1;32m--> 756\u001b[1;33m         \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m         \u001b[0mpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m  \u001b[0mpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args, **kw)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\TensorFlow\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2764\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[0;32m   2765\u001b[0m                            self.tk.splitlist(\n\u001b[1;32m-> 2766\u001b[1;33m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[0;32m   2767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2768\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from snake_env import Snake\n",
    "from plot_script import plot_result1\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Deep Q Network \"\"\"\n",
    "\n",
    "    def __init__(self, env, params):\n",
    "\n",
    "        self.action_space = env.action_space\n",
    "        self.state_space = env.state_space\n",
    "        self.epsilon = params['epsilon'] \n",
    "        self.gamma = params['gamma'] \n",
    "        self.batch_size = params['batch_size'] \n",
    "        self.epsilon_min = params['epsilon_min'] \n",
    "        self.epsilon_decay = params['epsilon_decay'] \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.layer_sizes = params['layer_sizes']\n",
    "        self.memory = deque(maxlen=2500)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        for i in range(len(self.layer_sizes)):\n",
    "            if i == 0:\n",
    "                model.add(Dense(self.layer_sizes[i], input_shape=(self.state_space,), activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(self.layer_sizes[i], activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='softmax'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    " \n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def train_dqn(episode, env):\n",
    "\n",
    "    sum_of_rewards = []\n",
    "    agent = DQN(env, params)\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, env.state_space))\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            # print(action)\n",
    "            prev_state = state\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, env.state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if params['batch_size'] > 1:\n",
    "                agent.replay()\n",
    "            if done:\n",
    "                print(f'final state before dying: {str(prev_state)}')\n",
    "                print(f'episode: {e+1}/{episode}, score: {score}')\n",
    "                break\n",
    "        sum_of_rewards.append(score)\n",
    "    return sum_of_rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    params = dict()\n",
    "    params['name'] = None\n",
    "    params['epsilon'] = 1\n",
    "    params['gamma'] = .95\n",
    "    params['batch_size'] = 500\n",
    "    params['epsilon_min'] = .01\n",
    "    params['epsilon_decay'] = 1/100\n",
    "    params['learning_rate'] = 0.00013629\n",
    "    params['layer_sizes'] = [128, 128, 128]\n",
    "    \n",
    "\n",
    "    results = dict()\n",
    "    ep = 100\n",
    "\n",
    "    # for batchsz in [1, 10, 100, 1000]:\n",
    "    #     print(batchsz)\n",
    "    #     params['batch_size'] = batchsz\n",
    "    #     nm = ''\n",
    "    #     params['name'] = f'Batchsize {batchsz}'\n",
    "    env_infos = {'States: only walls':{'state_space':'no body knowledge'}, 'States: direction 0 or 1':{'state_space':''}, 'States: coordinates':{'state_space':'coordinates'}, 'States: no direction':{'state_space':'no direction'}}\n",
    "\n",
    "    # for key in env_infos.keys():\n",
    "    #     params['name'] = key\n",
    "    #     env_info = env_infos[key]\n",
    "    #     print(env_info)\n",
    "    #     env = Snake(env_info=env_info)\n",
    "    env = Snake()\n",
    "    sum_of_rewards = train_dqn(ep, env)\n",
    "    results[params['name']] = sum_of_rewards\n",
    "    \n",
    "    plot_result1(results, direct=True, k=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-professor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
