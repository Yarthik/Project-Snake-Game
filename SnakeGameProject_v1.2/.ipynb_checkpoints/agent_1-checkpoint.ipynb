{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "###agent_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-spoke",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from snake_env.ipynb\n",
      "importing Jupyter notebook from plot_script.ipynb\n",
      "final state before dying: [[1 0 0 1 0 0 1 0 0 0 0 1]]\n",
      "episode: 1/50, score: -100\n",
      "final state before dying: [[0 0 1 1 1 0 0 0 0 0 0 1]]\n",
      "episode: 2/50, score: -110\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 3/50, score: -109\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 1 0 0 0]]\n",
      "episode: 4/50, score: -95\n",
      "final state before dying: [[1 0 0 1 0 0 1 0 0 0 1 0]]\n",
      "episode: 5/50, score: -105\n",
      "final state before dying: [[1 0 0 1 0 0 1 0 0 0 1 0]]\n",
      "episode: 6/50, score: -109\n",
      "final state before dying: [[0 0 1 1 1 0 0 0 1 0 0 0]]\n",
      "episode: 7/50, score: -113\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 0 0 1 0]]\n",
      "episode: 8/50, score: -109\n",
      "final state before dying: [[0 0 1 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 9/50, score: -111\n",
      "final state before dying: [[0 1 1 0 0 0 0 1 0 0 0 1]]\n",
      "episode: 10/50, score: -81\n",
      "final state before dying: [[1 1 0 0 0 0 1 0 0 0 1 0]]\n",
      "episode: 11/50, score: -83\n",
      "final state before dying: [[0 1 1 0 1 0 0 0 1 0 0 0]]\n",
      "episode: 12/50, score: -96\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 0 0 1 0]]\n",
      "episode: 13/50, score: -108\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 0 0 1 0]]\n",
      "episode: 14/50, score: -115\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 0 0 1 0]]\n",
      "episode: 15/50, score: -102\n",
      "final state before dying: [[0 0 1 1 0 0 1 0 0 0 0 1]]\n",
      "episode: 16/50, score: 318\n",
      "final state before dying: [[0 1 0 0 1 1 0 0 0 1 0 0]]\n",
      "episode: 17/50, score: 260\n",
      "final state before dying: [[0 1 1 0 0 0 1 1 0 0 1 0]]\n",
      "episode: 18/50, score: 492\n",
      "final state before dying: [[0 0 1 1 1 0 0 1 0 0 0 1]]\n",
      "episode: 19/50, score: 226\n",
      "final state before dying: [[1 0 0 1 1 0 0 1 0 0 0 1]]\n",
      "episode: 20/50, score: 391\n",
      "final state before dying: [[1 1 0 0 1 0 0 0 0 1 0 0]]\n",
      "episode: 21/50, score: 93\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from snake_env import Snake\n",
    "from plot_script import plot_result1\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Deep Q Network \"\"\"\n",
    "\n",
    "    def __init__(self, env, params):\n",
    "\n",
    "        self.action_space = env.action_space\n",
    "        self.state_space = env.state_space\n",
    "        self.epsilon = params['epsilon'] \n",
    "        self.gamma = params['gamma'] \n",
    "        self.batch_size = params['batch_size'] \n",
    "        self.epsilon_min = params['epsilon_min'] \n",
    "        self.epsilon_decay = params['epsilon_decay'] \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.layer_sizes = params['layer_sizes']\n",
    "        self.memory = deque(maxlen=2500)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        for i in range(len(self.layer_sizes)):\n",
    "            if i == 0:\n",
    "                model.add(Dense(self.layer_sizes[i], input_shape=(self.state_space,), activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(self.layer_sizes[i], activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='softmax'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    " \n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def train_dqn(episode, env):\n",
    "\n",
    "    sum_of_rewards = []\n",
    "    agent = DQN(env, params)\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, env.state_space))\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            # print(action)\n",
    "            prev_state = state\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, env.state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if params['batch_size'] > 1:\n",
    "                agent.replay()\n",
    "            if done:\n",
    "                print(f'final state before dying: {str(prev_state)}')\n",
    "                print(f'episode: {e+1}/{episode}, score: {score}')\n",
    "                break\n",
    "        sum_of_rewards.append(score)\n",
    "    return sum_of_rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    params = dict()\n",
    "    params['name'] = None\n",
    "    params['epsilon'] = 1\n",
    "    params['gamma'] = .95\n",
    "    params['batch_size'] = 500\n",
    "    params['epsilon_min'] = .01\n",
    "    params['epsilon_decay'] = .995\n",
    "    params['learning_rate'] = 0.00025\n",
    "    params['layer_sizes'] = [128, 128, 128]\n",
    "\n",
    "    results = dict()\n",
    "    ep = 50\n",
    "\n",
    "    # for batchsz in [1, 10, 100, 1000]:\n",
    "    #     print(batchsz)\n",
    "    #     params['batch_size'] = batchsz\n",
    "    #     nm = ''\n",
    "    #     params['name'] = f'Batchsize {batchsz}'\n",
    "    env_infos = {'States: only walls':{'state_space':'no body knowledge'}, 'States: direction 0 or 1':{'state_space':''}, 'States: coordinates':{'state_space':'coordinates'}, 'States: no direction':{'state_space':'no direction'}}\n",
    "\n",
    "    # for key in env_infos.keys():\n",
    "    #     params['name'] = key\n",
    "    #     env_info = env_infos[key]\n",
    "    #     print(env_info)\n",
    "    #     env = Snake(env_info=env_info)\n",
    "    env = Snake()\n",
    "    sum_of_rewards = train_dqn(ep, env)\n",
    "    results[params['name']] = sum_of_rewards\n",
    "    \n",
    "    plot_result1(results, direct=True, k=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-professor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
